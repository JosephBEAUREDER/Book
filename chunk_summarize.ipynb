{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of all chunk : 1339\n",
      "Title: L'ordre matériel du savoir, Author: Françoise Waquet\n",
      "Title: Histoire du structuralisme, Tome 2, Author: François Dosse\n",
      "Title: Histoire du structuralisme, Tome 1, Author: François Dosse\n",
      "Title: L’interprétation sociologique des rêves, Author: Bernard Lahire\n",
      "Title: L'ordre matériel du savoir, Author: Françoise Waquet\n",
      "Title: L’interprétation sociologique des rêves, Author: Bernard Lahire\n",
      "Title: Franz Kafka, Chapitre 1, Author: Bernard Lahire\n",
      "Title: Histoire du structuralisme, Tome 2, Author: François Dosse\n",
      "Title: L'ordre matériel du savoir, Author: Françoise Waquet\n",
      "Title: L’interprétation sociologique des rêves, Author: Bernard Lahire\n",
      "Title: L’interprétation sociologique des rêves, Author: Bernard Lahire\n",
      "Title: Histoire du structuralisme, Tome 2, Author: François Dosse\n",
      "Title: Franz Kafka, Chapitre 1, Author: Bernard Lahire\n",
      "Title: Histoire du structuralisme, Tome 1, Author: François Dosse\n",
      "Title: L’interprétation sociologique des rêves, Author: Bernard Lahire\n",
      "Title: L'ordre matériel du savoir, Author: Françoise Waquet\n",
      "Title: L'ordre matériel du savoir, Author: Françoise Waquet\n",
      "Title: Histoire du structuralisme, Tome 2, Author: François Dosse\n",
      "Title: Histoire du structuralisme, Tome 2, Author: François Dosse\n",
      "Title: Franz Kafka, Chapitre 1, Author: Bernard Lahire\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import openai\n",
    "from semantic_text_splitter import TextSplitter\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def split_and_merge_text(text, min_size=1500, max_size=3000):\n",
    "    \"\"\"\n",
    "    Splits the text using a TextSplitter and merges smaller chunks to meet size constraints.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be split.\n",
    "        min_size (int): The minimum size for each chunk.\n",
    "        max_size (int): The maximum size for each chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of merged chunks that meet the size constraints.\n",
    "    \"\"\"\n",
    "    # Initialize the splitter with the chunk size range\n",
    "    splitter = TextSplitter((min_size, max_size))\n",
    "\n",
    "    # Split the text into initial chunks\n",
    "    initial_chunks = splitter.chunks(text)\n",
    "\n",
    "    # Merge chunks smaller than the minimum size\n",
    "    merged_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for chunk in initial_chunks:\n",
    "        if len(current_chunk) + len(chunk) <= max_size:\n",
    "            current_chunk += chunk\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                merged_chunks.append(current_chunk)\n",
    "            current_chunk = chunk\n",
    "\n",
    "    if current_chunk:\n",
    "        merged_chunks.append(current_chunk)\n",
    "\n",
    "    # Ensure all chunks meet the minimum size requirement\n",
    "    final_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for chunk in merged_chunks:\n",
    "        if len(chunk) < min_size:\n",
    "            current_chunk += chunk\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                final_chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "            final_chunks.append(chunk)\n",
    "\n",
    "    if current_chunk:\n",
    "        final_chunks.append(current_chunk)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def extract_title_and_author(text):\n",
    "    \"\"\"\n",
    "    Extracts the title and author from the first few lines of a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing title and author information.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the title and author as strings.\n",
    "    \"\"\"\n",
    "    title = None\n",
    "    author = None\n",
    "\n",
    "    # Split the text into lines and check the first few lines\n",
    "    lines = text.splitlines()\n",
    "    for line in lines[:5]:  # Look at the first few lines for these fields\n",
    "        if line.startswith(\"Titre :\"):\n",
    "            title = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"Auteur :\"):\n",
    "            author = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    return title, author\n",
    "\n",
    "# Function to retrieve a specific insight from a chunk\n",
    "def retrieve_insight(chunk, author, title):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Use the appropriate model\n",
    "        prompt=(\n",
    "            f\"\"\"Le texte suivant est extrait de \"{title}\", écrit par {author}.\n",
    "            En mettant l'accent sur la perspective de {author}, identifie l'idée générale qu'il exprime selon son point de vue.\n",
    "            Ensuite, décompose cette idée en étapes successives, chacune correspondant à une phrase courte.\n",
    "            Chaque phrase doit décrire une étape claire et concise pour arriver à l'idée générale, tout en s'appuyant sur des détails du texte.\n",
    "            Si une phrase est trop longue, divise-la en deux phrases courtes.\n",
    "            Voici le texte :\\n\\n {chunk}\\n\\n\n",
    "            Étapes :\"\"\"\n",
    "        ),\n",
    "        max_tokens=500,  # Limit the length of the insight\n",
    "        temperature=0.7,  # Adjust for creativity (lower = more focused)\n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "def save_insight_to_json(title, author, insight, key, output_file):\n",
    "    \"\"\"\n",
    "    Saves insights into a JSON file with a given key, appending to existing data.\n",
    "    \"\"\"\n",
    "    # Parse the insight into individual points\n",
    "    points = [line.strip() for line in insight.strip().split('\\n') if line]\n",
    "\n",
    "    # Construct the JSON structure for the new entry\n",
    "    new_data = {\n",
    "        str(key): {\n",
    "            \"Title and author\": f\"{title}, {author}\",\n",
    "            \"Content\": points\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Load existing data if the file exists\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    # Update the existing data with the new entry\n",
    "    data.update(new_data)\n",
    "\n",
    "    # Save the updated data back to the file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Initialize the \"all_chunks\" variable to store data\n",
    "all_chunks = []\n",
    "\n",
    "# Path to the folder containing text files\n",
    "folder_path = \"Book\"\n",
    "\n",
    "# Loop through all .txt files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        # Read the text file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Create the chunks for this file\n",
    "        chunks = split_and_merge_text(text, min_size=1500, max_size=3000)\n",
    "\n",
    "        # Get the title and author from the text\n",
    "        title, author = extract_title_and_author(text)\n",
    "\n",
    "        # Add chunks to all_chunks with their associated title and author\n",
    "        for chunk in chunks:\n",
    "            all_chunks.append({\n",
    "                \"title\": title,\n",
    "                \"author\": author,\n",
    "                \"chunk\": chunk\n",
    "            })\n",
    "\n",
    "\n",
    "print(\"Lenght of all chunk : \" + str(len(all_chunks)))\n",
    "\n",
    "\n",
    "# Randomly select 20 chunks from \"all_chunks\"\n",
    "random_chunks = random.sample(all_chunks, min(20, len(all_chunks)))\n",
    "\n",
    "# Assuming all_chunks is already defined\n",
    "titles_and_authors = [{\"title\": chunk[\"title\"], \"author\": chunk[\"author\"]} for chunk in random_chunks]\n",
    "\n",
    "# Display the unique titles and authors\n",
    "for item in titles_and_authors:\n",
    "    print(f\"Title: {item['title']}, Author: {item['author']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "invalid model ID",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(random_chunks, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Retrieve an insight from the chunk\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     insight \u001b[38;5;241m=\u001b[39m retrieve_insight(chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m'\u001b[39m], chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m], chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Save the insight to a JSON file\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     save_insight_to_json(chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m], chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m], insight, i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsights.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 83\u001b[0m, in \u001b[0;36mretrieve_insight\u001b[1;34m(chunk, author, title)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_insight\u001b[39m(chunk, author, title):\n\u001b[1;32m---> 83\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     84\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Use the appropriate model\u001b[39;00m\n\u001b[0;32m     85\u001b[0m         prompt\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mLe texte suivant est extrait de \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, écrit par \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauthor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124m            En mettant l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccent sur la perspective de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauthor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, identifie l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midée générale qu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mil exprime selon son point de vue.\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124m            Ensuite, décompose cette idée en étapes successives, chacune correspondant à une phrase courte.\u001b[39m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124m            Chaque phrase doit décrire une étape claire et concise pour arriver à l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midée générale, tout en s\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappuyant sur des détails du texte.\u001b[39m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124m            Si une phrase est trop longue, divise-la en deux phrases courtes.\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124m            Ecrit le nom de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauteur et cite le titre du livre dans la première étape si nécessaire.\u001b[39m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124m            Voici le texte :\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124m            Étapes :\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         ),\n\u001b[0;32m     95\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,  \u001b[38;5;66;03m# Limit the length of the insight\u001b[39;00m\n\u001b[0;32m     96\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,  \u001b[38;5;66;03m# Adjust for creativity (lower = more focused)\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\Lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    702\u001b[0m             result\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\Lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: invalid model ID"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(random_chunks, start=1):\n",
    "    # Retrieve an insight from the chunk\n",
    "    insight = retrieve_insight(chunk['chunk'], chunk['author'], chunk['title'])\n",
    "    \n",
    "    # Save the insight to a JSON file\n",
    "    save_insight_to_json(chunk['title'], chunk['author'], insight, i, \"insights.json\")\n",
    "    print(f\"Saved insight {i} to insights.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Bernard Lahire',\n",
       "  'Franz Kafka, Chapitre 1',\n",
       "  'Titre : Franz Kafka, Chapitre 1\\nAuteur : Bernard Lahire\\n\\n\\nChapitre 1\\nL’enfermement dans le champLa seule manière de se défaire réellement de problèmes scientifiques, si l’on considère qu’une théorie sociologique est essentiellement un univers cohérent de problèmes-solutions articulés, c’est de les affronter, de les faire travailler, de les soumettre à examens, pour finalement les dépasser en découvrant leurs limites de validité et leur champ de pertinence. Partant d’une telle conception de la pratique scientifique, on ne peut qu’être d’accord avec l’analyse de Thomas S. Kuhn selon laquelle «\\u2005seules les investigations fermement enracinées dans la tradition scientifique contemporaine ont une chance de briser cette tradition et de donner naissance à une nouvelle1\\u2005». Et l’épistémologue ajoutait à la suite cette proposition d’une grande justesse : «\\u2005Le savant productif doit être un traditionaliste qui aime à s’adonner à des jeux complexes gouvernés par des règles préétablies, pour être un innovateur efficace qui découvre de nouvelles règles et de nouvelles pièces avec lesquelles il peut continuer à jouer2.\\u2005» Prolongeant une œuvre, on s’en détache fatalement, peu à peu, pour formuler de nouveaux problèmes et créer ses voies propres de résolution. Mais c’est tout le contraire de la stratégie qui consiste à laisser en plan l’adversaire en désertant le terrain pour créer son propre jeu en faisant tout pour faire préférer, au plus grand nombre possible, le nouveau jeu.\\n\\nFaire le choix déterminé (dans les deux sens du terme) de la stratégie de confrontation (vs stratégie de désertion), c’est toutefois prendre le risque de s’attirer les foudres de camps opposés, plutôt que de cumuler les faveurs, et de n’avoir pour lecteurs compréhensifs que le public des francs-tireurs de tous bords et de toutes disciplines. La chance d’être lu, et de l’être avec le minimum de patience nécessaire pour que la lecture soit correcte et ne soit pas le simple effet d’une projection d’a priori sur l’auteur du texte, est ainsi très faible. Mais un tel risque est à prendre lorsqu’on vise moins à plaire aux parties en présence qu’à convaincre à plus long terme de l’intérêt scientifique d’une démarche.'},\n",
       " {'Bernard Lahire',\n",
       "  'Franz Kafka, Chapitre 1',\n",
       "  'Impossible donc de parler de la création littéraire en sociologue sans se situer explicitement par rapport à une théorie du champ littéraire, qui est une déclinaison particulière de la théorie générale des champs. C’est d’autant plus difficile que cette dernière affiche comme ambition de penser autant les dimensions du monde littéraire les plus classiquement reconnues comme «\\u2005sociales\\u2005» (les trajectoires sociales et littéraires d’écrivains, la structuration de l’espace des positions littéraires et des luttes pour la domination symbolique, la sociologie historique des institutions littéraires telles que les maisons d’édition, les collections ou les revues, les stratégies éditoriales, les mouvements, courants ou écoles littéraires, les manifestes et manifestations littéraires de toutes sortes, le rôle de tous ceux qui — des éditeurs aux critiques en passant par l’institution scolaire, les médias et tous les pourvoyeurs de prix littéraires — contribuent à faire la valeur des œuvres) que les dimensions les plus spécifiquement littéraires des œuvres (thématiques, compositionnelles, stylistiques). Pierre Bourdieu affirmait même que «\\u2005la notion de champ permet de dépasser l’opposition entre lecture interne et analyse externe sans rien perdre des acquis et des exigences de ces deux approches, traditionnellement perçues comme inconciliables3\\u2005».'},\n",
       " {'Bernard Lahire',\n",
       "  'Franz Kafka, Chapitre 1',\n",
       "  'Les commentaires, positifs ou négatifs, portant sur une théorie aussi générale souffrent habituellement d’un certain nombre de défauts. Tout d’abord, les commentateurs ne distinguent pas toujours suffisamment ce qui est de l’ordre de l’affirmation d’un certain nombre de principes ou de directions théoriques et ce qui relève de leur mise en œuvre effective. De ce point de vue, on peut dire, sans faire offense aux utilisateurs de ses concepts, que Pierre Bourdieu a plus affirmé le dépassement des approches internes et externes, formelles et sociologiques, qu’il ne l’a réellement prouvé par des actes précis de recherche. Or, tout sociologue d’enquête sait bien qu’entre les principes et les concrétisations théoriques, méthodologiques et empiriques de ces principes, il y a parfois un gouffre. Le parcours, semé d’embûches, et notamment de limitations empiriques, qui mène des principes à l’étude circonscrite et fouillée amène souvent à la reformulation du problème initial, à la modification partielle des objectifs ou même à l’abandon des idées de départ. Le principe qui devrait guider tout examinateur d’une œuvre est celui qui consiste à ne croire que ce qu’il voit et à situer le débat sur le terrain de ce qui est fait, plutôt que sur celui des affirmations concernant ce que l’on prétend être en mesure de faire grâce au modèle théorique en question. Dans la réalité des travaux, la sociologie du champ littéraire a prouvé sa fécondité essentiellement en tant que sociologie des producteurs plutôt que comme sociologie des productions. Et lorsqu’elle parle des œuvres, elle est d’abord et avant tout une sociologie de la production sociale de la valeur des œuvres (des qualités littéraires qui leur sont collectivement attribuées et de leur degré de légitimité littéraire)4 et presque jamais une sociologie de la création littéraire (en tant qu’étude consacrée aux œuvres mêmes et à leur fabrication).'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Initialize the \"all_chunks\" variable to store data\n",
    "all_chunks = []\n",
    "\n",
    "# Path to the folder containing text files\n",
    "folder_path = \"Book\"\n",
    "\n",
    "# Loop through all .txt files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        # Read the text file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Create the chunks for this file\n",
    "        chunks = split_and_merge_text(text, min_size=1500, max_size=3000)\n",
    "\n",
    "        # Get the title and author from the text\n",
    "        title, author = extract_title_and_author(text)\n",
    "\n",
    "        # Add chunks to all_chunks with their associated title and author\n",
    "        for chunk in chunks:\n",
    "            all_chunks.append({\n",
    "                title,\n",
    "                author,\n",
    "                chunk\n",
    "            })\n",
    "\n",
    "# Check the first few entries to confirm structure\n",
    "all_chunks[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Françoise Waquet\n",
      "L'ordre matériel du savoir\n"
     ]
    }
   ],
   "source": [
    "# Extract title and author from the text\n",
    "with open('Book/LOrdre matériel du savoir.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "title = None\n",
    "author = None\n",
    "\n",
    "# Extract title and author\n",
    "for line in lines[:5]:  # Look at the first few lines for these fields\n",
    "    if line.startswith(\"Titre :\"):\n",
    "        title = line.split(\":\", 1)[1].strip()\n",
    "    elif line.startswith(\"Auteur :\"):\n",
    "        author = line.split(\":\", 1)[1].strip()\n",
    "        \n",
    "print(author)\n",
    "\n",
    "print(title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
