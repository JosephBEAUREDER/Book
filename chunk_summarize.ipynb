{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of all chunk : 2669\n",
      "Title: L’interprétation sociologique des rêves, Author: Bernard Lahire, Size: 2908 characters\n",
      "Title: L'ordre matériel du savoir, Author: Françoise Waquet, Size: 1881 characters\n",
      "Title: La question anthropologique (Cours 1954-1955), Author: Michel Foucault, Size: 1849 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 2244 characters\n",
      "Title: Histoire du structuralisme, Tome 2, Author: François Dosse, Size: 2148 characters\n",
      "Title: L'ordre matériel du savoir, Author: Françoise Waquet, Size: 2538 characters\n",
      "Title: La fabrique des sciences sociales, Author: Johann Michel, Size: 1940 characters\n",
      "Title: Histoire du structuralisme, Tome 1, Author: François Dosse, Size: 1517 characters\n",
      "Title: L’interprétation sociologique des rêves, Author: Bernard Lahire, Size: 2553 characters\n",
      "Title: La question anthropologique (Cours 1954-1955), Author: Michel Foucault, Size: 1689 characters\n",
      "Title: Histoire du structuralisme, Tome 1, Author: François Dosse, Size: 2565 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 2731 characters\n",
      "Title: La question anthropologique (Cours 1954-1955), Author: Michel Foucault, Size: 2013 characters\n",
      "Title: Histoire du structuralisme, Tome 1, Author: François Dosse, Size: 2658 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 1984 characters\n",
      "Title: L’interprétation sociologique des rêves, Author: Bernard Lahire, Size: 2567 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 2329 characters\n",
      "Title: L'ordre matériel du savoir, Author: Françoise Waquet, Size: 1956 characters\n",
      "Title: L’interprétation sociologique des rêves, Author: Bernard Lahire, Size: 2031 characters\n",
      "Title: La fabrique des sciences sociales, Author: Johann Michel, Size: 1868 characters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import openai\n",
    "from semantic_text_splitter import TextSplitter\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def split_and_merge_text(text, min_size=1500, max_size=3000):\n",
    "    \"\"\"\n",
    "    Splits the text using a TextSplitter and merges smaller chunks to meet size constraints.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be split.\n",
    "        min_size (int): The minimum size for each chunk.\n",
    "        max_size (int): The maximum size for each chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of merged chunks that meet the size constraints.\n",
    "    \"\"\"\n",
    "    # Initialize the splitter with the chunk size range\n",
    "    splitter = TextSplitter((min_size, max_size))\n",
    "\n",
    "    # Split the text into initial chunks\n",
    "    initial_chunks = splitter.chunks(text)\n",
    "\n",
    "    # Merge chunks smaller than the minimum size\n",
    "    merged_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for chunk in initial_chunks:\n",
    "        if len(current_chunk) + len(chunk) <= max_size:\n",
    "            current_chunk += chunk\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                merged_chunks.append(current_chunk)\n",
    "            current_chunk = chunk\n",
    "\n",
    "    if current_chunk:\n",
    "        merged_chunks.append(current_chunk)\n",
    "\n",
    "    # Ensure all chunks meet the minimum size requirement\n",
    "    final_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for chunk in merged_chunks:\n",
    "        if len(chunk) < min_size:\n",
    "            current_chunk += chunk\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                final_chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "            final_chunks.append(chunk)\n",
    "\n",
    "    if current_chunk:\n",
    "        final_chunks.append(current_chunk)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def extract_title_and_author(text):\n",
    "    \"\"\"\n",
    "    Extracts the title and author from the first few lines of a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing title and author information.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the title and author as strings.\n",
    "    \"\"\"\n",
    "    title = None\n",
    "    author = None\n",
    "\n",
    "    # Split the text into lines and check the first few lines\n",
    "    lines = text.splitlines()\n",
    "    for line in lines[:5]:  # Look at the first few lines for these fields\n",
    "        if line.startswith(\"Titre :\"):\n",
    "            title = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"Auteur :\"):\n",
    "            author = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    return title, author\n",
    "\n",
    "# Function to retrieve a specific insight from a chunk\n",
    "def retrieve_insight(chunk, author, title):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",  # Use the appropriate model\n",
    "        prompt=(\n",
    "            f\"\"\"Le texte suivant est extrait de \"{title}\", écrit par {author}.\n",
    "            En mettant l'accent sur la perspective de {author}, identifie l'idée générale qu'il exprime selon son point de vue.\n",
    "            Ensuite, décompose cette idée en étapes successives, chacune correspondant à une phrase courte.\n",
    "            Chaque phrase doit décrire une étape claire et concise pour arriver à l'idée générale, tout en s'appuyant sur des détails du texte.\n",
    "            Si une phrase est trop longue, divise-la en deux phrases courtes.\n",
    "            Voici le texte :\\n\\n {chunk}\\n\\n\n",
    "            Étapes :\"\"\"\n",
    "        ),\n",
    "        max_tokens=500,  # Limit the length of the insight\n",
    "        temperature=0.7,  # Adjust for creativity (lower = more focused)\n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "def save_insight_to_json(title, author, insight, key, output_file):\n",
    "    \"\"\"\n",
    "    Saves insights into a JSON file with a given key, appending to existing data.\n",
    "    \"\"\"\n",
    "    # Parse the insight into individual points\n",
    "    points = [line.strip() for line in insight.strip().split('\\n') if line]\n",
    "\n",
    "    # Construct the JSON structure for the new entry\n",
    "    new_data = {\n",
    "        str(key): {\n",
    "            \"Title and author\": f\"{title}, {author}\",\n",
    "            \"Content\": points\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Load existing data if the file exists\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    # Update the existing data with the new entry\n",
    "    data.update(new_data)\n",
    "\n",
    "    # Save the updated data back to the file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Initialize the \"all_chunks\" variable to store data\n",
    "all_chunks = []\n",
    "\n",
    "# Path to the folder containing text files\n",
    "folder_path = \"Book\"\n",
    "\n",
    "# Loop through all .txt files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        # Read the text file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Create the chunks for this file\n",
    "        chunks = split_and_merge_text(text, min_size=1500, max_size=3000)\n",
    "\n",
    "        # Get the title and author from the text\n",
    "        title, author = extract_title_and_author(text)\n",
    "\n",
    "        # Add chunks to all_chunks with their associated title and author\n",
    "        for chunk in chunks:\n",
    "            all_chunks.append({\n",
    "                \"title\": title,\n",
    "                \"author\": author,\n",
    "                \"chunk\": chunk,\n",
    "                \"size\": len(chunk),\n",
    "            })\n",
    "\n",
    "\n",
    "print(\"Lenght of all chunk : \" + str(len(all_chunks)))\n",
    "\n",
    "\n",
    "# Randomly select 20 chunks from \"all_chunks\"\n",
    "random_chunks = random.sample(all_chunks, min(20, len(all_chunks)))\n",
    "\n",
    "# Assuming all_chunks is already defined\n",
    "titles_and_authors = [\n",
    "    {\"title\": chunk[\"title\"], \"author\": chunk[\"author\"], \"size\": chunk[\"size\"]}\n",
    "    for chunk in random_chunks\n",
    "]\n",
    "\n",
    "# Display the unique titles, authors, and sizes\n",
    "for item in titles_and_authors:\n",
    "    print(f\"Title: {item['title']}, Author: {item['author']}, Size: {item['size']} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved insight 1 to insights.json\n",
      "Saved insight 2 to insights.json\n",
      "Saved insight 3 to insights.json\n",
      "Saved insight 4 to insights.json\n",
      "Saved insight 5 to insights.json\n",
      "Saved insight 6 to insights.json\n",
      "Saved insight 7 to insights.json\n",
      "Saved insight 8 to insights.json\n",
      "Saved insight 9 to insights.json\n",
      "Saved insight 10 to insights.json\n",
      "Saved insight 11 to insights.json\n",
      "Saved insight 12 to insights.json\n",
      "Saved insight 13 to insights.json\n",
      "Saved insight 14 to insights.json\n",
      "Saved insight 15 to insights.json\n",
      "Saved insight 16 to insights.json\n",
      "Saved insight 17 to insights.json\n",
      "Saved insight 18 to insights.json\n",
      "Saved insight 19 to insights.json\n",
      "Saved insight 20 to insights.json\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(random_chunks, start=1):\n",
    "    # Retrieve an insight from the chunk\n",
    "    insight = retrieve_insight(chunk['chunk'], chunk['author'], chunk['title'])\n",
    "    \n",
    "    # Save the insight to a JSON file\n",
    "    save_insight_to_json(chunk['title'], chunk['author'], insight, i, \"insights.json\")\n",
    "    print(f\"Saved insight {i} to insights.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Bernard Lahire',\n",
       "  'Franz Kafka, Chapitre 1',\n",
       "  'Titre : Franz Kafka, Chapitre 1\\nAuteur : Bernard Lahire\\n\\n\\nChapitre 1\\nL’enfermement dans le champLa seule manière de se défaire réellement de problèmes scientifiques, si l’on considère qu’une théorie sociologique est essentiellement un univers cohérent de problèmes-solutions articulés, c’est de les affronter, de les faire travailler, de les soumettre à examens, pour finalement les dépasser en découvrant leurs limites de validité et leur champ de pertinence. Partant d’une telle conception de la pratique scientifique, on ne peut qu’être d’accord avec l’analyse de Thomas S. Kuhn selon laquelle «\\u2005seules les investigations fermement enracinées dans la tradition scientifique contemporaine ont une chance de briser cette tradition et de donner naissance à une nouvelle1\\u2005». Et l’épistémologue ajoutait à la suite cette proposition d’une grande justesse : «\\u2005Le savant productif doit être un traditionaliste qui aime à s’adonner à des jeux complexes gouvernés par des règles préétablies, pour être un innovateur efficace qui découvre de nouvelles règles et de nouvelles pièces avec lesquelles il peut continuer à jouer2.\\u2005» Prolongeant une œuvre, on s’en détache fatalement, peu à peu, pour formuler de nouveaux problèmes et créer ses voies propres de résolution. Mais c’est tout le contraire de la stratégie qui consiste à laisser en plan l’adversaire en désertant le terrain pour créer son propre jeu en faisant tout pour faire préférer, au plus grand nombre possible, le nouveau jeu.\\n\\nFaire le choix déterminé (dans les deux sens du terme) de la stratégie de confrontation (vs stratégie de désertion), c’est toutefois prendre le risque de s’attirer les foudres de camps opposés, plutôt que de cumuler les faveurs, et de n’avoir pour lecteurs compréhensifs que le public des francs-tireurs de tous bords et de toutes disciplines. La chance d’être lu, et de l’être avec le minimum de patience nécessaire pour que la lecture soit correcte et ne soit pas le simple effet d’une projection d’a priori sur l’auteur du texte, est ainsi très faible. Mais un tel risque est à prendre lorsqu’on vise moins à plaire aux parties en présence qu’à convaincre à plus long terme de l’intérêt scientifique d’une démarche.'},\n",
       " {'Bernard Lahire',\n",
       "  'Franz Kafka, Chapitre 1',\n",
       "  'Impossible donc de parler de la création littéraire en sociologue sans se situer explicitement par rapport à une théorie du champ littéraire, qui est une déclinaison particulière de la théorie générale des champs. C’est d’autant plus difficile que cette dernière affiche comme ambition de penser autant les dimensions du monde littéraire les plus classiquement reconnues comme «\\u2005sociales\\u2005» (les trajectoires sociales et littéraires d’écrivains, la structuration de l’espace des positions littéraires et des luttes pour la domination symbolique, la sociologie historique des institutions littéraires telles que les maisons d’édition, les collections ou les revues, les stratégies éditoriales, les mouvements, courants ou écoles littéraires, les manifestes et manifestations littéraires de toutes sortes, le rôle de tous ceux qui — des éditeurs aux critiques en passant par l’institution scolaire, les médias et tous les pourvoyeurs de prix littéraires — contribuent à faire la valeur des œuvres) que les dimensions les plus spécifiquement littéraires des œuvres (thématiques, compositionnelles, stylistiques). Pierre Bourdieu affirmait même que «\\u2005la notion de champ permet de dépasser l’opposition entre lecture interne et analyse externe sans rien perdre des acquis et des exigences de ces deux approches, traditionnellement perçues comme inconciliables3\\u2005».'},\n",
       " {'Bernard Lahire',\n",
       "  'Franz Kafka, Chapitre 1',\n",
       "  'Les commentaires, positifs ou négatifs, portant sur une théorie aussi générale souffrent habituellement d’un certain nombre de défauts. Tout d’abord, les commentateurs ne distinguent pas toujours suffisamment ce qui est de l’ordre de l’affirmation d’un certain nombre de principes ou de directions théoriques et ce qui relève de leur mise en œuvre effective. De ce point de vue, on peut dire, sans faire offense aux utilisateurs de ses concepts, que Pierre Bourdieu a plus affirmé le dépassement des approches internes et externes, formelles et sociologiques, qu’il ne l’a réellement prouvé par des actes précis de recherche. Or, tout sociologue d’enquête sait bien qu’entre les principes et les concrétisations théoriques, méthodologiques et empiriques de ces principes, il y a parfois un gouffre. Le parcours, semé d’embûches, et notamment de limitations empiriques, qui mène des principes à l’étude circonscrite et fouillée amène souvent à la reformulation du problème initial, à la modification partielle des objectifs ou même à l’abandon des idées de départ. Le principe qui devrait guider tout examinateur d’une œuvre est celui qui consiste à ne croire que ce qu’il voit et à situer le débat sur le terrain de ce qui est fait, plutôt que sur celui des affirmations concernant ce que l’on prétend être en mesure de faire grâce au modèle théorique en question. Dans la réalité des travaux, la sociologie du champ littéraire a prouvé sa fécondité essentiellement en tant que sociologie des producteurs plutôt que comme sociologie des productions. Et lorsqu’elle parle des œuvres, elle est d’abord et avant tout une sociologie de la production sociale de la valeur des œuvres (des qualités littéraires qui leur sont collectivement attribuées et de leur degré de légitimité littéraire)4 et presque jamais une sociologie de la création littéraire (en tant qu’étude consacrée aux œuvres mêmes et à leur fabrication).'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Initialize the \"all_chunks\" variable to store data\n",
    "all_chunks = []\n",
    "\n",
    "# Path to the folder containing text files\n",
    "folder_path = \"Book\"\n",
    "\n",
    "# Loop through all .txt files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        # Read the text file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Create the chunks for this file\n",
    "        chunks = split_and_merge_text(text, min_size=1500, max_size=3000)\n",
    "\n",
    "        # Get the title and author from the text\n",
    "        title, author = extract_title_and_author(text)\n",
    "\n",
    "        # Add chunks to all_chunks with their associated title and author\n",
    "        for chunk in chunks:\n",
    "            all_chunks.append({\n",
    "                title,\n",
    "                author,\n",
    "                chunk\n",
    "            })\n",
    "\n",
    "# Check the first few entries to confirm structure\n",
    "all_chunks[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Françoise Waquet\n",
      "L'ordre matériel du savoir\n"
     ]
    }
   ],
   "source": [
    "# Extract title and author from the text\n",
    "with open('Book/LOrdre matériel du savoir.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "title = None\n",
    "author = None\n",
    "\n",
    "# Extract title and author\n",
    "for line in lines[:5]:  # Look at the first few lines for these fields\n",
    "    if line.startswith(\"Titre :\"):\n",
    "        title = line.split(\":\", 1)[1].strip()\n",
    "    elif line.startswith(\"Auteur :\"):\n",
    "        author = line.split(\":\", 1)[1].strip()\n",
    "        \n",
    "print(author)\n",
    "\n",
    "print(title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
