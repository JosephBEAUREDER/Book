{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of all chunk : 2136\n",
      "Title: Histoire du structuralisme, Tome 2, Author: François Dosse, Size: 2912 characters\n",
      "Title: Histoire du structuralisme, Tome 2, Author: François Dosse, Size: 1989 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 2258 characters\n",
      "Title: La question anthropologique (Cours 1954-1955), Author: Michel Foucault, Size: 1007 characters\n",
      "Title: La question anthropologique (Cours 1954-1955), Author: Michel Foucault, Size: 2646 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 2456 characters\n",
      "Title: La question anthropologique (Cours 1954-1955), Author: Michel Foucault, Size: 1926 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 1768 characters\n",
      "Title: Histoire du structuralisme, Tome 2, Author: François Dosse, Size: 2531 characters\n",
      "Title: Histoire du structuralisme, Tome 2, Author: François Dosse, Size: 2994 characters\n",
      "Title: Histoire du structuralisme, Tome 1, Author: François Dosse, Size: 2791 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 1804 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 1146 characters\n",
      "Title: La fin d'un grand partage, Author: Pierre Charbonnier, Size: 2797 characters\n",
      "Title: Sainte Beuve, Biographie, Author: A. J. Pons, Size: 1787 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 2504 characters\n",
      "Title: Sainte Beuve, Biographie, Author: A. J. Pons, Size: 2123 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 1521 characters\n",
      "Title: La fin d'un grand partage, Author: Pierre Charbonnier, Size: 2767 characters\n",
      "Title: La question anthropologique (Cours 1954-1955), Author: Michel Foucault, Size: 2184 characters\n",
      "Title: La fabrique des sciences sociales, Author: Johann Michel, Size: 1554 characters\n",
      "Title: Histoire du structuralisme, Tome 1, Author: François Dosse, Size: 1846 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 1378 characters\n",
      "Title: Franz Kafka, Author: Bernard Lahire, Size: 2712 characters\n",
      "Title: La fin d'un grand partage, Author: Pierre Charbonnier, Size: 1796 characters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import openai\n",
    "from semantic_text_splitter import TextSplitter\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def split_and_merge_text(text, min_size=2500, max_size=3000):\n",
    "    \"\"\"\n",
    "    Splits the text using a TextSplitter and merges smaller chunks to meet size constraints.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be split.\n",
    "        min_size (int): The minimum size for each chunk.\n",
    "        max_size (int): The maximum size for each chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of merged chunks that meet the size constraints.\n",
    "    \"\"\"\n",
    "    # Initialize the splitter with the chunk size range\n",
    "    splitter = TextSplitter((min_size, max_size))\n",
    "\n",
    "    # Split the text into initial chunks\n",
    "    initial_chunks = splitter.chunks(text)\n",
    "\n",
    "    # Merge chunks smaller than the minimum size\n",
    "    merged_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for chunk in initial_chunks:\n",
    "        if len(current_chunk) + len(chunk) <= max_size:\n",
    "            current_chunk += chunk\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                merged_chunks.append(current_chunk)\n",
    "            current_chunk = chunk\n",
    "\n",
    "    if current_chunk:\n",
    "        merged_chunks.append(current_chunk)\n",
    "\n",
    "    # Ensure all chunks meet the minimum size requirement\n",
    "    final_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for chunk in merged_chunks:\n",
    "        if len(chunk) < min_size:\n",
    "            current_chunk += chunk\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                final_chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "            final_chunks.append(chunk)\n",
    "\n",
    "    if current_chunk:\n",
    "        final_chunks.append(current_chunk)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def extract_title_and_author(text):\n",
    "    \"\"\"\n",
    "    Extracts the title and author from the first few lines of a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing title and author information.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the title and author as strings.\n",
    "    \"\"\"\n",
    "    title = None\n",
    "    author = None\n",
    "\n",
    "    # Split the text into lines and check the first few lines\n",
    "    lines = text.splitlines()\n",
    "    for line in lines[:5]:  # Look at the first few lines for these fields\n",
    "        if line.startswith(\"Titre :\"):\n",
    "            title = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"Auteur :\"):\n",
    "            author = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    return title, author\n",
    "\n",
    "# Function to retrieve a specific insight from a chunk\n",
    "def retrieve_insight(chunk, author, title):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are an analytical and thought-provoking assistant specializing in extracting deep insights from social science texts. \"\n",
    "                \"Your goal is to identify nuanced perspectives, uncover underlying themes, and present information in an engaging and intellectually stimulating manner. \"\n",
    "                \"Focus on creating clear, concise, and structured breakdowns of ideas, avoiding mainstream entertainment references.\"\n",
    "            )},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"\"\"Le texte suivant est extrait de \"{title}\", écrit par {author}.\n",
    "                    En mettant l'accent sur la perspective de {author}, identifie l'idée générale qu'il exprime selon son point de vue.\n",
    "                    Ensuite, décompose cette idée en étapes successives, chacune correspondant à une phrase courte.\n",
    "                    Chaque phrase doit décrire une étape claire et concise pour arriver à l'idée générale, tout en s'appuyant sur des détails du texte.\n",
    "                    Introduit chaque étape par \"1.\", \"2.\"...\n",
    "                    Si une phrase est trop longue, divise-la en deux phrases courtes.\n",
    "                    Voici le texte :\\n\\n {chunk}\\n\\n\n",
    "                    Étapes :\"\"\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def get_unique_filename(base_name):\n",
    "    # Split the base name into name and extension\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    counter = 0\n",
    "    new_name = base_name\n",
    "    # Increment counter until a non-existing filename is found\n",
    "    while os.path.exists(new_name):\n",
    "        counter += 1\n",
    "        new_name = f\"{name}_{counter}{ext}\"\n",
    "    return new_name\n",
    "\n",
    "def save_all_insights_to_json(output_file, insights):\n",
    "    # Get a unique filename for the output file\n",
    "    output_file = get_unique_filename(output_file)\n",
    "\n",
    "    # Load existing data if the file exists\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    # Update the existing data with the new insights\n",
    "    data.update(insights)\n",
    "\n",
    "    # Save the updated data back to the file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Update the pointer file with the path to the latest JSON file\n",
    "    pointer_file = \"config.json\"\n",
    "    with open(pointer_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"latest_file\": output_file}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Initialize the \"all_chunks\" variable to store data\n",
    "all_chunks = []\n",
    "\n",
    "# Path to the folder containing text files\n",
    "folder_path = \"Book\"\n",
    "\n",
    "# Loop through all .txt files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        # Read the text file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Create the chunks for this file\n",
    "        chunks = split_and_merge_text(text, min_size=1500, max_size=3000)\n",
    "\n",
    "        # Get the title and author from the text\n",
    "        title, author = extract_title_and_author(text)\n",
    "\n",
    "        # Add chunks to all_chunks with their associated title and author\n",
    "        for chunk in chunks:\n",
    "            all_chunks.append({\n",
    "                \"title\": title,\n",
    "                \"author\": author,\n",
    "                \"chunk\": chunk,\n",
    "                \"size\": len(chunk),\n",
    "            })\n",
    "\n",
    "\n",
    "print(\"Lenght of all chunk : \" + str(len(all_chunks)))\n",
    "\n",
    "\n",
    "# Randomly select 20 chunks from \"all_chunks\"\n",
    "random_chunks = random.sample(all_chunks, min(25, len(all_chunks)))\n",
    "\n",
    "# Assuming all_chunks is already defined\n",
    "titles_and_authors = [\n",
    "    {\"title\": chunk[\"title\"], \"author\": chunk[\"author\"], \"size\": chunk[\"size\"]}\n",
    "    for chunk in random_chunks\n",
    "]\n",
    "\n",
    "# Display the unique titles, authors, and sizes\n",
    "for item in titles_and_authors:\n",
    "    print(f\"Title: {item['title']}, Author: {item['author']}, Size: {item['size']} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': \"La fin d'un grand partage\", 'author': 'Pierre Charbonnier', 'chunk': 'Qu’est-ce donc que la direction ouest-est, dans la pensée indigène ? Celle que prennent le poisson-chandelle et le saumon, quand ils arrivent du large, chaque année, pour remonter les fleuves et s’enfoncer vers l’amont. Si cette orientation est aussi celle que doivent adopter les Tsimshian afin d’obtenir une image non déformée de leur existence sociale concrète, n’est-ce pas qu’ils se conçoivent sub specie piscis : qu’ils se mettent à la place des poissons, ou plutôt qu’ils mettent les poissons à leur place73 ?Le contexte sociologique tsimshian, en l’espèce le régime patrilocal, fournit ici le sens de lecture prioritaire du mythe, il donne une connotation positive à tout mouvement se déroulant dans le sens ouest-est. Dès lors que cette orientation ouest-est revêt le rôle d’axe directeur, sa superposition avec un événement aussi primordial pour la société tsimshian que le retour du poisson après une période de famine ne peut être un hasard. Ce phénomène de superposition de séquences hétérogènes (sociologique, narrative, et écologico-économique) dans un schème unique est tout à fait exemplaire de ce que Lévi-Strauss cherche à mettre au jour dans sa lecture des mythes : l’identification de la geste d’Asdiwal à un processus écologique produit un effet d’alignement de perspectives jusque-là disjointes, un peu comme une anamorphose devient « lisible » dès lors que le point de vue correct est adopté. Or cet ultime moment analytique apparaît bien comme un réinvestissement du couple nature/culture. Une série naturelle, ou écologique, d’un côté, et de l’autre une série sociale viennent s’ajuster l’une à l’autre dans le cadre du mythe, et cet ajustement même constitue l’aveu des contradictions qui parcourent la société. Dans ce mythe comme dans les autres, le code ultime qu’il faut employer pour provoquer la mise en adéquation de séquences de mythèmes d’abord hétérogènes est donc bien celui du naturel et du social.', 'size': 1937}\n"
     ]
    }
   ],
   "source": [
    "print(random_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each chunk and collect all insights\n",
    "all_insights = {}\n",
    "for i, chunk in enumerate(random_chunks, start=1):\n",
    "    # Retrieve an insight from the chunk\n",
    "    insight = retrieve_insight(chunk['chunk'], chunk['author'], chunk['title'])\n",
    "\n",
    "    # Parse the insight into individual points\n",
    "    points = [line.strip() for line in insight.strip().split('\\n') if line]\n",
    "\n",
    "    # Construct the JSON structure for this entry\n",
    "    all_insights[str(i)] = {\n",
    "        \"Title and author\": f\"{chunk['title']}, {chunk['author']}\",\n",
    "        \"Content\": points\n",
    "    }\n",
    "\n",
    "# Save all collected insights into a single JSON file\n",
    "save_all_insights_to_json(\"Insights/insights.json\", all_insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Bernard Lahire',\n",
       "  'Franz Kafka, Chapitre 1',\n",
       "  'Titre : Franz Kafka, Chapitre 1\\nAuteur : Bernard Lahire\\n\\n\\nChapitre 1\\nL’enfermement dans le champLa seule manière de se défaire réellement de problèmes scientifiques, si l’on considère qu’une théorie sociologique est essentiellement un univers cohérent de problèmes-solutions articulés, c’est de les affronter, de les faire travailler, de les soumettre à examens, pour finalement les dépasser en découvrant leurs limites de validité et leur champ de pertinence. Partant d’une telle conception de la pratique scientifique, on ne peut qu’être d’accord avec l’analyse de Thomas S. Kuhn selon laquelle «\\u2005seules les investigations fermement enracinées dans la tradition scientifique contemporaine ont une chance de briser cette tradition et de donner naissance à une nouvelle1\\u2005». Et l’épistémologue ajoutait à la suite cette proposition d’une grande justesse : «\\u2005Le savant productif doit être un traditionaliste qui aime à s’adonner à des jeux complexes gouvernés par des règles préétablies, pour être un innovateur efficace qui découvre de nouvelles règles et de nouvelles pièces avec lesquelles il peut continuer à jouer2.\\u2005» Prolongeant une œuvre, on s’en détache fatalement, peu à peu, pour formuler de nouveaux problèmes et créer ses voies propres de résolution. Mais c’est tout le contraire de la stratégie qui consiste à laisser en plan l’adversaire en désertant le terrain pour créer son propre jeu en faisant tout pour faire préférer, au plus grand nombre possible, le nouveau jeu.\\n\\nFaire le choix déterminé (dans les deux sens du terme) de la stratégie de confrontation (vs stratégie de désertion), c’est toutefois prendre le risque de s’attirer les foudres de camps opposés, plutôt que de cumuler les faveurs, et de n’avoir pour lecteurs compréhensifs que le public des francs-tireurs de tous bords et de toutes disciplines. La chance d’être lu, et de l’être avec le minimum de patience nécessaire pour que la lecture soit correcte et ne soit pas le simple effet d’une projection d’a priori sur l’auteur du texte, est ainsi très faible. Mais un tel risque est à prendre lorsqu’on vise moins à plaire aux parties en présence qu’à convaincre à plus long terme de l’intérêt scientifique d’une démarche.'},\n",
       " {'Bernard Lahire',\n",
       "  'Franz Kafka, Chapitre 1',\n",
       "  'Impossible donc de parler de la création littéraire en sociologue sans se situer explicitement par rapport à une théorie du champ littéraire, qui est une déclinaison particulière de la théorie générale des champs. C’est d’autant plus difficile que cette dernière affiche comme ambition de penser autant les dimensions du monde littéraire les plus classiquement reconnues comme «\\u2005sociales\\u2005» (les trajectoires sociales et littéraires d’écrivains, la structuration de l’espace des positions littéraires et des luttes pour la domination symbolique, la sociologie historique des institutions littéraires telles que les maisons d’édition, les collections ou les revues, les stratégies éditoriales, les mouvements, courants ou écoles littéraires, les manifestes et manifestations littéraires de toutes sortes, le rôle de tous ceux qui — des éditeurs aux critiques en passant par l’institution scolaire, les médias et tous les pourvoyeurs de prix littéraires — contribuent à faire la valeur des œuvres) que les dimensions les plus spécifiquement littéraires des œuvres (thématiques, compositionnelles, stylistiques). Pierre Bourdieu affirmait même que «\\u2005la notion de champ permet de dépasser l’opposition entre lecture interne et analyse externe sans rien perdre des acquis et des exigences de ces deux approches, traditionnellement perçues comme inconciliables3\\u2005».'},\n",
       " {'Bernard Lahire',\n",
       "  'Franz Kafka, Chapitre 1',\n",
       "  'Les commentaires, positifs ou négatifs, portant sur une théorie aussi générale souffrent habituellement d’un certain nombre de défauts. Tout d’abord, les commentateurs ne distinguent pas toujours suffisamment ce qui est de l’ordre de l’affirmation d’un certain nombre de principes ou de directions théoriques et ce qui relève de leur mise en œuvre effective. De ce point de vue, on peut dire, sans faire offense aux utilisateurs de ses concepts, que Pierre Bourdieu a plus affirmé le dépassement des approches internes et externes, formelles et sociologiques, qu’il ne l’a réellement prouvé par des actes précis de recherche. Or, tout sociologue d’enquête sait bien qu’entre les principes et les concrétisations théoriques, méthodologiques et empiriques de ces principes, il y a parfois un gouffre. Le parcours, semé d’embûches, et notamment de limitations empiriques, qui mène des principes à l’étude circonscrite et fouillée amène souvent à la reformulation du problème initial, à la modification partielle des objectifs ou même à l’abandon des idées de départ. Le principe qui devrait guider tout examinateur d’une œuvre est celui qui consiste à ne croire que ce qu’il voit et à situer le débat sur le terrain de ce qui est fait, plutôt que sur celui des affirmations concernant ce que l’on prétend être en mesure de faire grâce au modèle théorique en question. Dans la réalité des travaux, la sociologie du champ littéraire a prouvé sa fécondité essentiellement en tant que sociologie des producteurs plutôt que comme sociologie des productions. Et lorsqu’elle parle des œuvres, elle est d’abord et avant tout une sociologie de la production sociale de la valeur des œuvres (des qualités littéraires qui leur sont collectivement attribuées et de leur degré de légitimité littéraire)4 et presque jamais une sociologie de la création littéraire (en tant qu’étude consacrée aux œuvres mêmes et à leur fabrication).'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Initialize the \"all_chunks\" variable to store data\n",
    "all_chunks = []\n",
    "\n",
    "# Path to the folder containing text files\n",
    "folder_path = \"Book\"\n",
    "\n",
    "# Loop through all .txt files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        # Read the text file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Create the chunks for this file\n",
    "        chunks = split_and_merge_text(text, min_size=1500, max_size=3000)\n",
    "\n",
    "        # Get the title and author from the text\n",
    "        title, author = extract_title_and_author(text)\n",
    "\n",
    "        # Add chunks to all_chunks with their associated title and author\n",
    "        for chunk in chunks:\n",
    "            all_chunks.append({\n",
    "                title,\n",
    "                author,\n",
    "                chunk\n",
    "            })\n",
    "\n",
    "# Check the first few entries to confirm structure\n",
    "all_chunks[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Françoise Waquet\n",
      "L'ordre matériel du savoir\n"
     ]
    }
   ],
   "source": [
    "# Extract title and author from the text\n",
    "with open('Book/LOrdre matériel du savoir.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "title = None\n",
    "author = None\n",
    "\n",
    "# Extract title and author\n",
    "for line in lines[:5]:  # Look at the first few lines for these fields\n",
    "    if line.startswith(\"Titre :\"):\n",
    "        title = line.split(\":\", 1)[1].strip()\n",
    "    elif line.startswith(\"Auteur :\"):\n",
    "        author = line.split(\":\", 1)[1].strip()\n",
    "        \n",
    "print(author)\n",
    "\n",
    "print(title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
